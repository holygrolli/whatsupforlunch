name: moritzbastei
on:
  workflow_dispatch:
    inputs:
      force_publish:
        description: execute publish job
        type: boolean
        default: false
  schedule:
    - cron: '0 9 * * 2'
permissions: 
  contents: write
  packages: write
  pull-requests: write
jobs:
  check:
    runs-on: ubuntu-latest
    container:
      image: ghcr.io/holygrolli/whatsupforlunch:main
    outputs:
      ts: ${{ steps.detectchange.outputs.ts }}
    steps:
      - name: checkout
        uses: actions/checkout@v3
        with:
          ref: >-
            ${{
              github.event_name == 'schedule' && 'main' || ''
            }}
      - name: check if site is updated
        id: detectchange
        shell: bash
        run: |
          curl -s https://www.moritzbastei.de/gastronomie-kneipe-bistro-drinks/ | \
            python locations/moritzbastei/check_site_update.py > data/moritzbastei/scraped_done.txt
          git config --system --add safe.directory $PWD
          git status -s | grep "moritzbastei/scraped_done.txt" && echo "::set-output name=ts::$(cat data/moritzbastei/scraped_done.txt)" || echo "no updates"
  download:
    needs: check
    if: ${{ needs.check.outputs.ts != '' || inputs.force_publish }}
    runs-on: ubuntu-latest
    container:
      image: ghcr.io/holygrolli/whatsupforlunch:main
    steps:
      - name: checkout
        uses: actions/checkout@v3
        with:
          ref: >-
            ${{
              github.event_name == 'schedule' && 'main' || ''
            }}
      - name: update change date
        if: ${{ needs.check.outputs.ts != '' }}
        shell: bash
        run: |
          echo "${{ needs.check.outputs.ts }}" > data/moritzbastei/scraped_done.txt
      - name: get new links
        shell: bash
        run: |
          WD=${PWD}
          mkdir -p tmp
          scrapy runspider locations/moritzbastei/scra.py --nolog > tmp/chatgpt_user.txt
      - name: process txt with chatgpt
        working-directory: tmp
        env:
          OPENAI_API_KEY: ${{secrets.OPENAI_API_KEY}}
        shell: bash
        run: |
          python ../locations/moritzbastei/process_chatgpt.py
          mkdir -p ../data/moritzbastei
          sed -n '/^\s*{$/,$p' chatgpt.json > final.json
          cp final.json ../data/moritzbastei/$(date -d"$(jq -r 'keys[] | select(test("\\d{4}-\\d{2}-\\d{2}"))' final.json | sort | head -n 1)" +%Y-%V).json
      - name: Archive all tmp files
        uses: actions/upload-artifact@v3
        with:
          name: moritzbastei-processed
          retention-days: 5
          path: |
            tmp
      - uses: tibdex/github-app-token@v2
        if: ${{ !github.event.act && (github.event_name != 'workflow_dispatch' || inputs.force_publish) }}
        id: generate-token
        with:
          app_id: ${{ secrets.TOKEN_APP_ID }}
          private_key: ${{ secrets.TOKEN_APP_PRIVATE_KEY }}
      - name: Create Pull Request
        if: ${{ !github.event.act && (github.event_name != 'workflow_dispatch' || (github.ref == 'refs/heads/main' && inputs.force_publish)) }}
        uses: peter-evans/create-pull-request@v5
        with:
          add-paths: |
            data
          branch: ${{ github.workflow }}-cron
          commit-message: "chore(data): automated scraping"
          labels: "auto-approve-cron"
          title: "${{ github.workflow }} data updates"
          token: ${{ steps.generate-token.outputs.token }}
